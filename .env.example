# LLM Configuration
# Required: API key for OpenAI-compatible service
LLM_API_KEY=your_api_key_here

# Optional: Model specification (default: "gpt-4o")
LLM_MODEL=gpt-4o

# Optional: Base URL for API (default: "https://api.openai.com/v1")
# For OpenRouter: https://openrouter.ai/api/v1
# For DeepSeek: https://api.deepseek.com
LLM_BASE_URL=https://api.openai.com/v1

# Optional: Output directory (default: "./output")
OUTPUT_DIR=./output

# Optional: Temperature for LLM generation (default: 0.1)
LLM_TEMPERATURE=0.1

# Optional: Maximum tokens per response (default: 4000)
LLM_MAX_TOKENS=4000